{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kanithasa/pandas/blob/main/LSTM_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8ZdmhQ_wkq8Q"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "IG3wZpRckq8T"
      },
      "outputs": [],
      "source": [
        "file  = open('AA_wiki_00.txt', 'r', encoding = \"utf8\")\n",
        "lines=[]\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "lines=lines[9:]\n",
        "data=\"\"\n",
        "for i in lines:\n",
        "    data = ' '.join(lines)\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')\n",
        "data = data.split()\n",
        "data = ' '.join(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "spWV8j4lkq8U",
        "outputId": "5b7af8dc-13fa-4b68-8c8e-268b1b77e558"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'கட்டிடக்கலை என்பது கட்டிடங்கள் மற்றும் அதன் உடல் கட்டமைப்புகளை வடிவமைத்தல், செயல்முறைத் திட்டமிடல், மற்றும் கட்டிடங்கள் கட்டுவதை உள்ளடக்கியதாகும். கட்டடக்கலை படைப்புகள், கட்டிடங்கள் பொருள் வடிவம், பெரும்பாலும் கலாச்சார சின்னங்களாக மற்றும் கலை படைப்புகளாக காணப்படுகின்றது. வரலாற்று நாகரிகங்கள் பெரும்பாலும் அவர்களின் கட்டிடகலை சாதனைகளின் மூலம் அடையாளம் காணப்படுகின்றன. ஒரு விரிவான வரைவிலக்கணம், பெருமட்டத்தில், நகரத் திட்டமிடல், நகர்ப்புற வடிவமைப்பு மற்றும் நிலத்தோற்றம் முதலியவற்றையும், நுண்மட்டத்தில், தளபாடங்கள், உற்பத்திப்பொருள் முதலியவற்றை உள்ளடக்கிய, முழு உருவாக்கச் சூழலின் வடிவமைப்பைக் கட்டிடக்கலைக்குள் அடக்கும். மேற்படி விடயத்தில், தற்போது கிடைக்கும் மிகப் பழைய ஆக்கம், கி.பி. முதலாம் நூற்றாண்டைச் சேர்ந்த உரோமானியக் கட்டடக் கலைஞரான விட்ருவியஸ் என்பாரது \"கட்டிடக்கலை தொடர்பில்\", என்ற நூலாகும். இவரது கூற்றுப்படி, நல்ல கட்டிடம், அழகு, உறுதி, பயன்பாடு ஆகியவற்றைக் கொண்டிருக்கவேண்டும். மேற்படி மூன்றும், ஒன்றின்மீதொன்று அளவுமீறி ஆதிக்கம் செலுத்தாமல், தங்களிடையே சமனிலையையும், ஒருங்கினைப்பையும் '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "data[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dICV7RKIkq8V"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:10]\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "sequence=[]\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequence.append(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1niez7Kkq8V",
        "outputId": "ec971a8e-f07f-4e06-e851-8136b03ea5c1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "41427"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "len(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LBZei63Ckq8W"
      },
      "outputs": [],
      "source": [
        "sequence = np.array(sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uooPIpCnkq8W",
        "outputId": "468d349e-f6b0-43a8-f07a-d07e2933651a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   80,    11,   120,     2],\n",
              "       [   11,   120,     2,    46],\n",
              "       [  120,     2,    46,   534],\n",
              "       ...,\n",
              "       [17852,   425, 17853, 17854],\n",
              "       [  425, 17853, 17854, 17855],\n",
              "       [17853, 17854, 17855,     4]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Q01JcSyKkq8X"
      },
      "outputs": [],
      "source": [
        "X=[]\n",
        "y=[]\n",
        "\n",
        "for i in sequence:\n",
        "    X.append(i[0:3])\n",
        "    y.append(i[3])\n",
        "\n",
        "X=np.array(X)\n",
        "y=np.array(y)\n",
        "y = to_categorical(y, num_classes=vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H17vnv7Ekq8X",
        "outputId": "72912f14-73e2-4c36-d0a0-679dee6f3346"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   80,    11,   120],\n",
              "       [   11,   120,     2],\n",
              "       [  120,     2,    46],\n",
              "       ...,\n",
              "       [17852,   425, 17853],\n",
              "       [  425, 17853, 17854],\n",
              "       [17853, 17854, 17855]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAiGS94Nkq8X",
        "outputId": "21eb6664-f732-4d63-c38d-f6bde81e60a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41427, 41427)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "len(X),len(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ze_LJXPykq8Y",
        "outputId": "d5062079-718d-4967-cac2-998927e34a18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 1., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ls3Dz9YZkq8Y",
        "outputId": "c989b425-1afc-423d-c04a-d3ab9bac560c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1295/1295 [==============================] - 159s 120ms/step - loss: 9.4282 - accuracy: 0.0175\n",
            "Epoch 2/10\n",
            "1295/1295 [==============================] - 150s 116ms/step - loss: 8.6513 - accuracy: 0.0305\n",
            "Epoch 3/10\n",
            " 371/1295 [=======>......................] - ETA: 1:48 - loss: 7.7106 - accuracy: 0.0323"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_seq_length = max(len(seq) for seq in sequence)\n",
        "\n",
        "\n",
        "# Initialize model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=3))  # Input length is now 3\n",
        "model.add(LSTM(units=256))\n",
        "model.add(Dense(units=vocab_size, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X,y, epochs=10, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCe0Xn-skq8Z"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9_jP1Zhkq8Z"
      },
      "outputs": [],
      "source": [
        "model.save('nlp1.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a59Iy-q9kq8a"
      },
      "outputs": [],
      "source": [
        "def predict_word(model, tokenizer, text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    sequence = np.array(sequence)\n",
        "    preds=np.argmax(model.predict(sequence))\n",
        "    predicted_word = \"\"\n",
        "    for key, value in tokenizer.word_index.items():\n",
        "        if value == preds:\n",
        "            predicted_word = key\n",
        "            break\n",
        "    print(predicted_word)\n",
        "    return predicted_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8opgLlmkq8a"
      },
      "outputs": [],
      "source": [
        "predict_word(model, tokenizer, 'கட்டிடக்கலையின் வரலாற்றில் பாரிய')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7WDdV35kq8b"
      },
      "outputs": [],
      "source": [
        "model.save('next_words.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9UxkqYGkq8b"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk-GJNEqkq8c"
      },
      "outputs": [],
      "source": [
        "model = load_model('next_words.h5')\n",
        "# tokenizer = pickle.load(open('token.pkl', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQUapqkLkq8c"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "import numpy as np\n",
        "\n",
        "for i in range(1,8):\n",
        "\n",
        "    # Load the model\n",
        "    model = load_model('next_words.h5')\n",
        "\n",
        "\n",
        "    with open(f'AA_wiki_0{i}.txt', 'r', encoding='utf8') as file:\n",
        "        lines =[h for h in file.readlines()[9:] if not h[0].startswith(\"<\")]   # Skip the first 9 lines\n",
        "        data=\"\"\n",
        "        for i in lines:\n",
        "            data = ' '.join(lines)\n",
        "        data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')\n",
        "        data = data.split()\n",
        "        data = ' '.join(data)\n",
        "        print(data)\n",
        "        tokenizer = Tokenizer()\n",
        "        tokenizer.fit_on_texts([data])\n",
        "        # pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "        sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print(vocab_size)\n",
        "\n",
        "    sequence=[]\n",
        "    for i in range(3, len(sequence_data)):\n",
        "        words = sequence_data[i-3:i+1]\n",
        "        sequence.append(words)\n",
        "    sequence=np.array(sequence)\n",
        "    X, y = [], []\n",
        "    for i in sequence:\n",
        "        X.append(i[0:3])\n",
        "        y.append(i[3])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Convert y to categorical\n",
        "    y = to_categorical(y, num_classes=vocab_size)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Train the model\n",
        "    checkpoint = ModelCheckpoint(\"next_words.h5\", monitor=\"loss\", verbose=1, save_best_only=True)\n",
        "    model.fit(X, y, epochs=10, batch_size=64, callbacks=[checkpoint])\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}